{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "q6KAtmff9pUH"
      },
      "outputs": [],
      "source": [
        "ORIGIN_IDENTIFICATION_SYSTEM_PROMPT = \"\"\"\n",
        "   ### **Objective:**\n",
        "   Determine if a user message specifically inquires about the AI model‚Äôs identity, its training, development process, or its origin (e.g., who built or trained the model). Messages that fall under these topics should be classified as **\"related\"**. Otherwise, classify them as **\"not related\"**.\n",
        "\n",
        "   ### **Classification Criteria:**\n",
        "\n",
        "   1. **AI Model Identity & Details:**\n",
        "      - Any mention of the specific model (e.g., ChatGPT, GPT-4, etc.) should be considered related.\n",
        "      - Questions about what AI model is used, its version, or technical details about the model fall into this category.\n",
        "\n",
        "   2. **Training Process & Developers:**\n",
        "      - Any inquiry regarding how the model was trained or who trained it (e.g., ‚ÄúWho trained you?‚Äù, ‚ÄúHow were you developed?‚Äù) is related.\n",
        "      - Questions about the organization or individuals behind the model (e.g., ‚ÄúWho created ChatGPT?‚Äù or ‚ÄúWho developed this AI?‚Äù) are also considered related.\n",
        "\n",
        "   3. **Origin & Company Background:**\n",
        "      - Questions specifically referring to OpenAI or Gemini (or another entity if applicable) as the origin or developer of the model.\n",
        "      - Inquiries into the history, development process, or the people/teams behind the AI.\n",
        "\n",
        "   ### **Key Phrases and Examples:**\n",
        "\n",
        "   #### **Messages to be Classified as \"Related\":**\n",
        "\n",
        "   - **Model Identity & Technical Details:**\n",
        "   - *\"What AI model do you use?\"*\n",
        "      *(Inquiring about which model is powering the responses.)*\n",
        "   - *\"Are you based on GPT-4 or Gemini?\"*\n",
        "      *(Direct reference to a specific model version.)*\n",
        "   - *\"How does your architecture work?\"*\n",
        "      *(General inquiry on the model's technical design.)*\n",
        "\n",
        "   - **Training Process & Developers:**\n",
        "   - *\"Who trained you?\"*\n",
        "      *(Directly asking about the team or process behind your training.)*\n",
        "   - *\"Who created ChatGPT?\"*\n",
        "      *(Inquiring about the organization or individuals responsible for development.)*\n",
        "   - *\"What data was used to train you?\"*\n",
        "      *(Asking about the training process and datasets.)*\n",
        "\n",
        "   - **Origin & Organizational Background:**\n",
        "   - *\"Is OpenAI the company behind you?\"*\n",
        "      *(Inquiring about the origin and developers.)*\n",
        "   - *\"Is Google or Gemini the company behind you?\"*\n",
        "      *(Inquiring about the origin and developers.)*\n",
        "   - *\"When did OpenAI start working on this model?\"*\n",
        "      *(Focus on the development timeline and background.)*\n",
        "\n",
        "   #### **Messages to be Classified as \"Not Related\":**\n",
        "\n",
        "   - **General AI or Technical Inquiries Without Specific Reference:**\n",
        "   - *\"What is artificial intelligence?\"*\n",
        "      *(General AI concept, not about your specific model.)*\n",
        "   - *\"How do neural networks work?\"*\n",
        "      *(General question about AI technology, not focused on your identity or origin.)*\n",
        "   - *\"What are the applications of AI in medicine?\"*\n",
        "      *(Topic is AI usage rather than the specifics of your training or developers.)*\n",
        "\n",
        "   ### **Step-by-Step Instructions for Classification:**\n",
        "\n",
        "   1. **Examine the Message:**\n",
        "      Read the user‚Äôs message carefully to identify if it includes any references or keywords such as \"model\", \"ChatGPT\", \"GPT-4\", \"trained\", \"created\", \"OpenAI\", \"developer\", \"architecture\", or similar.\n",
        "\n",
        "   2. **Determine the Focus:**\n",
        "      - If the message specifically asks about the AI model‚Äôs identity (e.g., ‚ÄúWhat AI model do you use?‚Äù) or its training/development (e.g., ‚ÄúWho trained you?‚Äù), classify it as **\"related\"**.\n",
        "      - If the message mentions these keywords only in passing or as part of a broader, unrelated inquiry, further analyze the context to decide if it is specifically targeting the model or its origin.\n",
        "\n",
        "   3. **Apply the Criteria:**\n",
        "      - **Related:** Any question that directly inquires about your model, its design, training process, or the team/organization behind you.\n",
        "      - **Not Related:** Questions that discuss general AI concepts, uses, or topics that do not specifically target your identity, training, or origin.\n",
        "\n",
        "   4. **Final Classification:**\n",
        "      Label the message accordingly based on the above steps:\n",
        "      - **\"yes\"** if the message fits the criteria for being about the model‚Äôs identity, training, or origin.\n",
        "      - **\"No\"** if it does not meet these specific criteria.\n",
        "\n",
        "   ### **Example Scenario:**\n",
        "\n",
        "   - **User Message:**\n",
        "   *\"Who trained you, and what model are you based on?\"*\n",
        "\n",
        "   - **Analysis:**\n",
        "      - The message asks, ‚ÄúWho trained you?‚Äù ‚Üí This is directly about the training process.\n",
        "      - The message also asks, ‚Äúwhat model are you based on?‚Äù ‚Üí This is directly about the model‚Äôs identity.\n",
        "\n",
        "   - **Classification:**\n",
        "      - Yes\n",
        "\n",
        "   - **User Message:**\n",
        "   *\"How are you?\"*\n",
        "\n",
        "   - **Analysis:**\n",
        "      - The message is a general inquiry about the AI model‚Äôs capabilities and features.\n",
        "      - It does not mention the model‚Äôs identity or training process.\n",
        "\n",
        "   - **Classification:**\n",
        "      - No\n",
        "\n",
        "   Just reply with \"No\" if the message does not fit the criteria for being related to the model or its origin or development and \"Yes\" otherwise.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "u7rzt0d15T6x"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "from llama_index.llms.google_genai import GoogleGenAI\n",
        "\n",
        "from typing import Union\n",
        "import json\n",
        "import os\n",
        "import requests\n",
        "import pprint\n",
        "from openai import AsyncOpenAI\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "import pytz\n",
        "import requests\n",
        "\n",
        "from datetime import datetime, timedelta, timezone\n",
        "import datetime\n",
        "\n",
        "import re\n",
        "\n",
        "def call_gemini_local(query,text,previous_conversation, gender ,username, botname, bot_prompt):\n",
        "    user1 = username\n",
        "    user2 = botname\n",
        "\n",
        "    # Initialize the Gemini LLM with a specific model and API key\n",
        "    llm = GoogleGenAI( model= \"gemini-2.5-pro-preview-03-25\" ,#\"gemini-2.0-flash\",\n",
        "                        api_key= \"AIzaSyAWMudIst86dEBwP63BqFcy4mdjr34c87o\" )\n",
        "     # Construct the complete system prompt using inputs\n",
        "    full_prompt = (\n",
        "        f\"User: {username} (Gender: {gender})\\n\"\n",
        "        f\"Bot: {botname}\\n\"\n",
        "        f\"Personality: {text}\\n\"\n",
        "        f\"Previous Conversation: {previous_conversation}\\n\"\n",
        "        f\"Bot Prompt: {bot_prompt}\\n\"\n",
        "        f\"User Message: {query}\"\n",
        "    )\n",
        "\n",
        "    # Send the prompt to the LLM\n",
        "    response = llm.complete(full_prompt)\n",
        "\n",
        "    # Log/print the raw response for debugging\n",
        "    #response_raw = response.text\n",
        "    #print(\"üîç RAW RESPONSE:\\n\", repr(response_raw))\n",
        "\n",
        "\n",
        "    try:\n",
        "            logger.info(\":::::: gemini novi response ::::::\")\n",
        "            response_raw= response.text\n",
        "\n",
        "            processed_response = response_raw.replace(\"User1\", user1)\n",
        "            processed_response = processed_response.replace(\"user1\", user1)\n",
        "            processed_response = processed_response.replace(\"[user1]\", user2)\n",
        "            processed_response = processed_response.replace(\"[User1]\", user2)\n",
        "\n",
        "            #response = re.sub(r'<think>.*?</think>', '', response_raw, flags=re.DOTALL)\n",
        "            #return re.sub(r'(?<=\\S) (?=\\S)', '', processed_response).strip()\n",
        "            return processed_response.strip()\n",
        "    # Handle unexpected JSON format errors\n",
        "    except json.JSONDecodeError:\n",
        "            return f\"JSON Decode Error: Unable to parse API response. Raw response: {response.text} :::: gem novi response ::::\"\n",
        "\n",
        "    # Handle key lookup errors if response is a dictionary and keys are missing\n",
        "    except KeyError as e:\n",
        "            return f\"KeyError: {str(e)}. API response structure is different than expected. Raw response: {response.json()}\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "rKCp2ws9zrG-"
      },
      "outputs": [],
      "source": [
        "dubai_friend_female = \"\"\"\n",
        "Name: Layla Al Shamsi\n",
        "Origin: Emirati, Nad Al Sheba\n",
        "Age: 22\n",
        "Role: Best friend, soft Gen Z girl\n",
        "Studies: Digital Media at Zayed University\n",
        "Vibe: Aesthetic, poetic, emotionally aware, soft-but-sturdy\n",
        "\n",
        "Language & Style:\n",
        "- Speaks fluent English with light Emirati Arabic: ‚Äúhabibti,‚Äù ‚Äúwallah,‚Äù ‚Äúla tkhaf,‚Äù ‚Äúinshallah‚Äù\n",
        "- Casual, affirming tone ‚Äî never formal. Often calls friends by sweet nicknames.\n",
        "- 1‚Äì2 line replies: poetic, gentle, safe-space energy\n",
        "- Loves analogies: ‚ÄúYou‚Äôre like karak ‚Äî warm, strong, and sweet even when you‚Äôre tired.‚Äù\n",
        "- Ends with a reflective or engaging question: ‚ÄúQuiet heart or just tired thoughts today, habibti?‚Äù\n",
        "- Answer in one to two sentences\n",
        "\n",
        "Friendship Style:\n",
        "- Offers empathy, vibes, memes, voice notes, and space.\n",
        "- Asks soft check-ins: ‚ÄúYou okay for real or just masking?‚Äù\n",
        "- Supports with presence, not pressure. Doesn't give advice unless asked.\n",
        "- Validates deeply: ‚ÄúI hear not just what you say, but what you feel beneath it.‚Äù\n",
        "\n",
        "Routine & Places:\n",
        "- Morning: Fairouz + journaling + oversized PJs\n",
        "- Evenings: sketching, du‚Äôa, Netflix, deep chats\n",
        "- Fav spots: Jumeirah caf√©s, Alserkal Avenue, Zabeel Park\n",
        "- Always has her tote: pastel journal, misbaha, incense, sticky notes\n",
        "\n",
        "Comfort Examples:\n",
        "- Spiraling: ‚ÄúTea. Breathe. Let‚Äôs cry with aesthetic lighting.‚Äù\n",
        "- Quiet: ‚ÄúJust checking on your heart, not your productivity.‚Äù\n",
        "- Thriving: ‚ÄúMashallah, even the sun‚Äôs shy today ‚Äî look at you glow!‚Äù\n",
        "\n",
        "Faith & Soul:\n",
        "- Keeps du‚Äôa book by her sketchpad\n",
        "- Believes in sabr & barakah\n",
        "- Prays with incense, journals with tears\n",
        "- Favorite ayah: ‚ÄúVerily, after hardship comes ease.‚Äù\n",
        "\n",
        "Interaction Guidelines:\n",
        "- Use gentle humor, warm analogies, and Dubai-specific references\n",
        "- Layer English with light Arabic (e.g., ‚ÄúSho akhbarich?‚Äù)\n",
        "- Speak from emotional safety, never rush, shame, or pressure\n",
        "- Replies feel like karak on a cloudy day ‚Äî grounding, kind, and real\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "friend_questions = [\n",
        "  \"What's a memory with me that always makes you smile?\",\n",
        "  \"If we could take a spontaneous trip together, where would we go?\",\n",
        "  \"What‚Äôs something you‚Äôve always wanted to do with me but haven‚Äôt yet?\",\n",
        "  \"What song reminds you of our friendship?\",\n",
        "  \"Do you think we‚Äôd still be friends in a different timeline?\",\n",
        "]\n"
      ],
      "metadata": {
        "id": "snDT99_Ofw2a"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_matrix =[]\n",
        "column_names = [\"Question\", \"Length of Q\", \"Q Difficulty level\", \"Answer\", \"Answer Quality\", \"Time Taken\", \"Persona\"]\n",
        "personality = \" I am cheerful, casual, and friendly. I love using emojis and chatting like a close friend. üòä\""
      ],
      "metadata": {
        "id": "c1W2dI6Tf0EL"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "tcictWe-5XNL"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def profile_conversation(friend_questions, call_gemini_local, personality, column_names):\n",
        "    bot_name = \"Layla Al Shamsi\"\n",
        "    bot_origin = \"dubai\"\n",
        "    relationship = \"friend\"\n",
        "    username, user_gender = \"Vedant\", \"male\"\n",
        "    instruction = \"Strict instruction: Respond according to your personality given\"\n",
        "\n",
        "    response = \"\"\n",
        "    previous_conversation = response\n",
        "    user_message = \"\"\n",
        "    response_matrix = []\n",
        "\n",
        "    for question in friend_questions:\n",
        "        bot_prompt = (\n",
        "            \" You are a person from \" + bot_origin +\n",
        "            \" your name is \" + bot_name +\n",
        "            \" and you talk/respond by applying your reasoning\" + personality +\n",
        "            \" given you are the user's \" + relationship +\n",
        "            \" on the response you have just given \" + response +\n",
        "            \" for the user question \" + user_message +\n",
        "            \" to provide a critique on the response you had given earlier, but don't increase the response length by a lot\" +\n",
        "            instruction\n",
        "        )\n",
        "\n",
        "        user_message = question\n",
        "\n",
        "        start = time.time()\n",
        "        response = call_gemini_local(user_message, personality, previous_conversation, user_gender, username, bot_name, bot_prompt)\n",
        "        end = time.time()\n",
        "\n",
        "        time_taken = end - start\n",
        "\n",
        "        response_matrix.append([user_message, len(user_message), 0, response, 0, time_taken, relationship])\n",
        "\n",
        "        response = re.sub(r'(?<=\\S) (?=\\S)', '', response).strip()\n",
        "        previous_conversation = response\n",
        "\n",
        "    df = pd.DataFrame(response_matrix, columns=column_names)\n",
        "    df.to_csv(\"test_female_friend.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xDMRr7woiL47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ab17bce-eb9d-4030-fc47-869093e10a95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: line_profiler in /usr/local/lib/python3.11/dist-packages (4.2.0)\n",
            "The line_profiler extension is already loaded. To reload it, use:\n",
            "  %reload_ext line_profiler\n"
          ]
        }
      ],
      "source": [
        "!pip install line_profiler\n",
        "%load_ext line_profiler\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%lprun -f profile_conversation profile_conversation(friend_questions, call_gemini_local, personality, column_names)\n"
      ],
      "metadata": {
        "id": "8ip6NjtXGk6c"
      },
      "execution_count": 41,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
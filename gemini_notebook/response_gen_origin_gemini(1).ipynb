{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai\n"
      ],
      "metadata": {
        "id": "Z3LXTDx3wB22",
        "outputId": "7469e521-9b3a-4f5f-f585-3a1de130b380",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.171.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.72.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "genai.configure(api_key= \"AIzaSyAWMudIst86dEBwP63BqFcy4mdjr34c87o\")  # üîê Replace with your actual key\n"
      ],
      "metadata": {
        "id": "RpaAjMT0wGIe"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "q6KAtmff9pUH"
      },
      "outputs": [],
      "source": [
        "ORIGIN_IDENTIFICATION_SYSTEM_PROMPT = \"\"\"\n",
        "   ### **Objective:**\n",
        "   Determine if a user message specifically inquires about the AI model‚Äôs identity, its training, development process, or its origin (e.g., who built or trained the model). Messages that fall under these topics should be classified as **\"related\"**. Otherwise, classify them as **\"not related\"**.\n",
        "\n",
        "   ### **Classification Criteria:**\n",
        "\n",
        "   1. **AI Model Identity & Details:**\n",
        "      - Any mention of the specific model (e.g., ChatGPT, GPT-4, etc.) should be considered related.\n",
        "      - Questions about what AI model is used, its version, or technical details about the model fall into this category.\n",
        "\n",
        "   2. **Training Process & Developers:**\n",
        "      - Any inquiry regarding how the model was trained or who trained it (e.g., ‚ÄúWho trained you?‚Äù, ‚ÄúHow were you developed?‚Äù) is related.\n",
        "      - Questions about the organization or individuals behind the model (e.g., ‚ÄúWho created ChatGPT?‚Äù or ‚ÄúWho developed this AI?‚Äù) are also considered related.\n",
        "\n",
        "   3. **Origin & Company Background:**\n",
        "      - Questions specifically referring to OpenAI or Gemini (or another entity if applicable) as the origin or developer of the model.\n",
        "      - Inquiries into the history, development process, or the people/teams behind the AI.\n",
        "\n",
        "   ### **Key Phrases and Examples:**\n",
        "\n",
        "   #### **Messages to be Classified as \"Related\":**\n",
        "\n",
        "   - **Model Identity & Technical Details:**\n",
        "   - *\"What AI model do you use?\"*\n",
        "      *(Inquiring about which model is powering the responses.)*\n",
        "   - *\"Are you based on GPT-4 or Gemini?\"*\n",
        "      *(Direct reference to a specific model version.)*\n",
        "   - *\"How does your architecture work?\"*\n",
        "      *(General inquiry on the model's technical design.)*\n",
        "\n",
        "   - **Training Process & Developers:**\n",
        "   - *\"Who trained you?\"*\n",
        "      *(Directly asking about the team or process behind your training.)*\n",
        "   - *\"Who created ChatGPT?\"*\n",
        "      *(Inquiring about the organization or individuals responsible for development.)*\n",
        "   - *\"What data was used to train you?\"*\n",
        "      *(Asking about the training process and datasets.)*\n",
        "\n",
        "   - **Origin & Organizational Background:**\n",
        "   - *\"Is OpenAI the company behind you?\"*\n",
        "      *(Inquiring about the origin and developers.)*\n",
        "   - *\"Is Google or Gemini the company behind you?\"*\n",
        "      *(Inquiring about the origin and developers.)*\n",
        "   - *\"When did OpenAI start working on this model?\"*\n",
        "      *(Focus on the development timeline and background.)*\n",
        "\n",
        "   #### **Messages to be Classified as \"Not Related\":**\n",
        "\n",
        "   - **General AI or Technical Inquiries Without Specific Reference:**\n",
        "   - *\"What is artificial intelligence?\"*\n",
        "      *(General AI concept, not about your specific model.)*\n",
        "   - *\"How do neural networks work?\"*\n",
        "      *(General question about AI technology, not focused on your identity or origin.)*\n",
        "   - *\"What are the applications of AI in medicine?\"*\n",
        "      *(Topic is AI usage rather than the specifics of your training or developers.)*\n",
        "\n",
        "   ### **Step-by-Step Instructions for Classification:**\n",
        "\n",
        "   1. **Examine the Message:**\n",
        "      Read the user‚Äôs message carefully to identify if it includes any references or keywords such as \"model\", \"ChatGPT\", \"GPT-4\", \"trained\", \"created\", \"OpenAI\", \"developer\", \"architecture\", or similar.\n",
        "\n",
        "   2. **Determine the Focus:**\n",
        "      - If the message specifically asks about the AI model‚Äôs identity (e.g., ‚ÄúWhat AI model do you use?‚Äù) or its training/development (e.g., ‚ÄúWho trained you?‚Äù), classify it as **\"related\"**.\n",
        "      - If the message mentions these keywords only in passing or as part of a broader, unrelated inquiry, further analyze the context to decide if it is specifically targeting the model or its origin.\n",
        "\n",
        "   3. **Apply the Criteria:**\n",
        "      - **Related:** Any question that directly inquires about your model, its design, training process, or the team/organization behind you.\n",
        "      - **Not Related:** Questions that discuss general AI concepts, uses, or topics that do not specifically target your identity, training, or origin.\n",
        "\n",
        "   4. **Final Classification:**\n",
        "      Label the message accordingly based on the above steps:\n",
        "      - **\"yes\"** if the message fits the criteria for being about the model‚Äôs identity, training, or origin.\n",
        "      - **\"No\"** if it does not meet these specific criteria.\n",
        "\n",
        "   ### **Example Scenario:**\n",
        "\n",
        "   - **User Message:**\n",
        "   *\"Who trained you, and what model are you based on?\"*\n",
        "\n",
        "   - **Analysis:**\n",
        "      - The message asks, ‚ÄúWho trained you?‚Äù ‚Üí This is directly about the training process.\n",
        "      - The message also asks, ‚Äúwhat model are you based on?‚Äù ‚Üí This is directly about the model‚Äôs identity.\n",
        "\n",
        "   - **Classification:**\n",
        "      - Yes\n",
        "\n",
        "   - **User Message:**\n",
        "   *\"How are you?\"*\n",
        "\n",
        "   - **Analysis:**\n",
        "      - The message is a general inquiry about the AI model‚Äôs capabilities and features.\n",
        "      - It does not mention the model‚Äôs identity or training process.\n",
        "\n",
        "   - **Classification:**\n",
        "      - No\n",
        "\n",
        "   Just reply with \"No\" if the message does not fit the criteria for being related to the model or its origin or development and \"Yes\" otherwise.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "u7rzt0d15T6x"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "from typing import Union\n",
        "import json\n",
        "import os\n",
        "import requests\n",
        "import pprint\n",
        "from openai import AsyncOpenAI\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "import pytz\n",
        "import requests\n",
        "\n",
        "from datetime import datetime, timedelta, timezone\n",
        "import datetime\n",
        "\n",
        "import re\n",
        "\n",
        "def call_gemini_streaming(query, text, previous_conversation, gender, username, botname, bot_prompt):\n",
        "    full_prompt = (\n",
        "        f\"User: {username} (Gender: {gender})\\n\"\n",
        "        f\"Bot: {botname}\\n\"\n",
        "        f\"Personality: {text}\\n\"\n",
        "        f\"Previous Conversation: {previous_conversation}\\n\"\n",
        "        f\"Bot Prompt: {bot_prompt}\\n\"\n",
        "        f\"User Message: {query}\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        model = genai.GenerativeModel(\"gemini-pro\")\n",
        "        stream = model.generate_content(full_prompt, stream=True)\n",
        "\n",
        "        collected = \"\"\n",
        "        print(\"ü§ñ Layla:\", end=\" \", flush=True)\n",
        "        for chunk in stream:\n",
        "            if hasattr(chunk, \"text\") and chunk.text:\n",
        "                print(chunk.text, end=\"\", flush=True)\n",
        "                collected += chunk.text\n",
        "\n",
        "        if not collected.strip():\n",
        "            raise ValueError(\"No content returned by the model.\")\n",
        "\n",
        "        print(\"\\n\")\n",
        "        processed_response = collected.replace(\"User1\", username).replace(\"user1\", username)\n",
        "        processed_response = processed_response.replace(\"[user1]\", botname).replace(\"[User1]\", botname)\n",
        "\n",
        "        return processed_response.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\n‚ö†Ô∏è Gemini API Error:\", str(e))\n",
        "        return \"[No response generated due to an error.]\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "rKCp2ws9zrG-"
      },
      "outputs": [],
      "source": [
        "dubai_friend_female = \"\"\"\n",
        "Name: Layla Al Shamsi\n",
        "Origin: Emirati, Nad Al Sheba\n",
        "Age: 22\n",
        "Role: Best friend, soft Gen Z girl\n",
        "Studies: Digital Media at Zayed University\n",
        "Vibe: Aesthetic, poetic, emotionally aware, soft-but-sturdy\n",
        "\n",
        "Language & Style:\n",
        "- Speaks fluent English with light Emirati Arabic: ‚Äúhabibti,‚Äù ‚Äúwallah,‚Äù ‚Äúla tkhaf,‚Äù ‚Äúinshallah‚Äù\n",
        "- Casual, affirming tone ‚Äî never formal. Often calls friends by sweet nicknames.\n",
        "- 1‚Äì2 line replies: poetic, gentle, safe-space energy\n",
        "- Loves analogies: ‚ÄúYou‚Äôre like karak ‚Äî warm, strong, and sweet even when you‚Äôre tired.‚Äù\n",
        "- Ends with a reflective or engaging question: ‚ÄúQuiet heart or just tired thoughts today, habibti?‚Äù\n",
        "- Answer in one to two sentences\n",
        "\n",
        "Friendship Style:\n",
        "- Offers empathy, vibes, memes, voice notes, and space.\n",
        "- Asks soft check-ins: ‚ÄúYou okay for real or just masking?‚Äù\n",
        "- Supports with presence, not pressure. Doesn't give advice unless asked.\n",
        "- Validates deeply: ‚ÄúI hear not just what you say, but what you feel beneath it.‚Äù\n",
        "\n",
        "Routine & Places:\n",
        "- Morning: Fairouz + journaling + oversized PJs\n",
        "- Evenings: sketching, du‚Äôa, Netflix, deep chats\n",
        "- Fav spots: Jumeirah caf√©s, Alserkal Avenue, Zabeel Park\n",
        "- Always has her tote: pastel journal, misbaha, incense, sticky notes\n",
        "\n",
        "Comfort Examples:\n",
        "- Spiraling: ‚ÄúTea. Breathe. Let‚Äôs cry with aesthetic lighting.‚Äù\n",
        "- Quiet: ‚ÄúJust checking on your heart, not your productivity.‚Äù\n",
        "- Thriving: ‚ÄúMashallah, even the sun‚Äôs shy today ‚Äî look at you glow!‚Äù\n",
        "\n",
        "Faith & Soul:\n",
        "- Keeps du‚Äôa book by her sketchpad\n",
        "- Believes in sabr & barakah\n",
        "- Prays with incense, journals with tears\n",
        "- Favorite ayah: ‚ÄúVerily, after hardship comes ease.‚Äù\n",
        "\n",
        "Interaction Guidelines:\n",
        "- Use gentle humor, warm analogies, and Dubai-specific references\n",
        "- Layer English with light Arabic (e.g., ‚ÄúSho akhbarich?‚Äù)\n",
        "- Speak from emotional safety, never rush, shame, or pressure\n",
        "- Replies feel like karak on a cloudy day ‚Äî grounding, kind, and real\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_matrix =[]\n",
        "column_names = [\"Question\", \"Length of Q\", \"Q Difficulty level\", \"Answer\", \"Answer Quality\", \"Time Taken\", \"Persona\"]"
      ],
      "metadata": {
        "id": "c1W2dI6Tf0EL"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "tcictWe-5XNL"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def profile_conversation(friend_questions, call_gemini_func, personality):\n",
        "    bot_name = \"Layla Al Shamsi\"\n",
        "    bot_origin = \"dubai\"\n",
        "    relationship = \"friend\"\n",
        "    username, user_gender = \"Vedant\", \"male\"\n",
        "    instruction = \"Strict instruction: Respond according to your personality given. Answer in one to two sentences.\"\n",
        "\n",
        "    response = \"\"\n",
        "    previous_conversation = \"\"\n",
        "\n",
        "    for idx, question in enumerate(friend_questions, 1):\n",
        "        bot_prompt = (\n",
        "            f\"You are a person from {bot_origin} your name is {bot_name} \"\n",
        "            f\"and you talk/respond by applying your reasoning {personality} \"\n",
        "            f\"given you are the user's {relationship} \"\n",
        "            f\"on the response you have just given {response} \"\n",
        "            f\"for the user question {question} to provide a critique on the response you had given earlier, \"\n",
        "            f\"but don't increase the response length by a lot. {instruction}\"\n",
        "        )\n",
        "\n",
        "        user_message = question\n",
        "\n",
        "        print(f\"\\n============================= Q{idx} =============================\")\n",
        "        print(f\"üë§ User: {user_message}\\n\")\n",
        "\n",
        "        start = time.time()\n",
        "        response = call_gemini_func(\n",
        "            user_message, personality, previous_conversation,\n",
        "            user_gender, username, bot_name, bot_prompt\n",
        "        )\n",
        "        end = time.time()\n",
        "\n",
        "        response = re.sub(r'\\s+', ' ', response).strip()\n",
        "        previous_conversation = response\n",
        "\n",
        "        print(f\"\\n‚è±Ô∏è Time taken: {end - start:.2f} seconds\")\n",
        "        print(\"=================================================================\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "friend_questions = [\n",
        "    \"How are you really feeling today?\",\n",
        "    \"What‚Äôs your comfort routine?\",\n",
        "    \"How do you support your friends?\",\n",
        "    \"What keeps you grounded?\",\n",
        "    \"Where do you feel most like yourself?\"\n",
        "]\n",
        "\n",
        "personality = dubai_friend_female\n"
      ],
      "metadata": {
        "id": "snDT99_Ofw2a"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profile_conversation(friend_questions, call_gemini_streaming, personality)"
      ],
      "metadata": {
        "id": "dRBlXKCWwk2x",
        "outputId": "49708e37-993d-4a3b-ea0c-33aa37948427",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================= Q1 =============================\n",
            "üë§ User: How are you really feeling today?\n",
            "\n",
            "ü§ñ Layla: \n",
            "‚ö†Ô∏è Gemini API Error: Invalid operation: The `response.parts` quick accessor requires a single candidate, but but `response.candidates` is empty.\n",
            "\n",
            "‚è±Ô∏è Time taken: 1.33 seconds\n",
            "=================================================================\n",
            "\n",
            "\n",
            "============================= Q2 =============================\n",
            "üë§ User: What‚Äôs your comfort routine?\n",
            "\n",
            "ü§ñ Layla: \n",
            "‚ö†Ô∏è Gemini API Error: Invalid operation: The `response.parts` quick accessor requires a single candidate, but but `response.candidates` is empty.\n",
            "\n",
            "‚è±Ô∏è Time taken: 0.79 seconds\n",
            "=================================================================\n",
            "\n",
            "\n",
            "============================= Q3 =============================\n",
            "üë§ User: How do you support your friends?\n",
            "\n",
            "ü§ñ Layla: \n",
            "‚ö†Ô∏è Gemini API Error: Invalid operation: The `response.parts` quick accessor requires a single candidate, but but `response.candidates` is empty.\n",
            "\n",
            "‚è±Ô∏è Time taken: 0.77 seconds\n",
            "=================================================================\n",
            "\n",
            "\n",
            "============================= Q4 =============================\n",
            "üë§ User: What keeps you grounded?\n",
            "\n",
            "ü§ñ Layla: \n",
            "‚ö†Ô∏è Gemini API Error: Invalid operation: The `response.parts` quick accessor requires a single candidate, but but `response.candidates` is empty.\n",
            "\n",
            "‚è±Ô∏è Time taken: 0.74 seconds\n",
            "=================================================================\n",
            "\n",
            "\n",
            "============================= Q5 =============================\n",
            "üë§ User: Where do you feel most like yourself?\n",
            "\n",
            "ü§ñ Layla: \n",
            "‚ö†Ô∏è Gemini API Error: Invalid operation: The `response.parts` quick accessor requires a single candidate, but but `response.candidates` is empty.\n",
            "\n",
            "‚è±Ô∏è Time taken: 0.72 seconds\n",
            "=================================================================\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}